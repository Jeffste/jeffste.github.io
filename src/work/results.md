---
title: Results
eleventyExcludeFromCollections: true
featuredImage: /images/card baseline@2x.png
workIntro: One of my primary responsibilities was the results dashboard. It is an exciting page for lots of our clients. After surveying, they can finally analyze and learn how their organization or team is doing. In this case study, I briefly explain my process toward the new results dashboard.
---

<figure>
  <img src="/images/ef-001.png">
  <figcaption>Preview of the new results dashboard</figcaption>
</figure>

### Background
We build Employee Listening Solutions. The feedback journey always starts with listening. Our consultants create (together with the client) surveys that suit the needs and goals of our clients.<br/>
When the runtime ends and most employees complete the survey, most managers can't wait to see the results. Curious how their team performed compared to other groups and what they can do to grow.<br/>
And now the user lands at the results dashboard—a project I single-handedly designed last year. But of course, supported with excellent feedback along the way.
<br/>
### Business requirements
The Results Dashboard is there to view surveys results. The CPO decided on a new Results Dashboard as the existing dashboard was deemed inadequate due to lacking features, usability, design, and performance. The new Results Dashboard will also consider surveys with less than 5 participants as those currently are left blank due to confidentiality reasons.

<figure>
  <img src="/images/ef-002.png">
  <figcaption>The old results dashboard</figcaption>
</figure>

### User research
A few months before starting this project, an external agency facilitated a design sprint with our designers and internal stakeholders. In the Understand Phase, they gathered useful information and created four personas.


<figure>
  <img src="/images/ef-003.png">
  <figcaption>Personas that represent our users</figcaption>
</figure>

### Wireframing
Before I started with wireframing first investigated all the ins and outs of the current results dashboard & the surveys. How do we ask the questions, and how do we display them when the results are available. Creating wireframes allowed me to explore lots of different ideas rapidly. I shared the early iterations with the team to gather feedback and brainstorm ideas.

<figure>
  <img src="/images/ef-004.png">
  <figcaption>Quick online wireframes made with Invision Freehand</figcaption>
</figure>

###   Designing
When I was confident about the flow, I started designing screens for the results dashboard. First, I created responsive columns and an 8px grid system. Later on, this will help me align elements and prepare the other breakpoints. <br/>
Before I start designing, I look at the surroundings pages. What do the users experience before they land here? Those pages are outside the scope, but I'm focussing mainly on the visual hierarchy across pages. The page's importance compared to the others will affect how the page will look. <br/>

<video width="1100" height="800" autoplay loop muted playsinline>
  <source src="/images/responsive.mp4" type="video/mp4" />
</video>

This method will prevent me from creating similar-looking pages and, if done right, result in an excellent visual hierarchy across pages from important → to less important. The results dashboard is off course, one that can stand out, as it contains the content lots of users are waiting for and often is a starting point when login.

<figure>
  <img src="/images/ef-005.png">
  <figcaption>Zoom out to see if the visual hierarchy makes sense</figcaption>
</figure>

### Component Library
To remain consistent, I made use of our component library. The component library, or how we call it "Engage design system," is created and maintained by the design team. And as we all know, it's never finished.

<figure>
  <img src="/images/ef-006.png">
  <figcaption>The library of components helps to stay consistent</figcaption>
</figure>





### Prototyping
During the wireframing phase, I created a short scenario to help me make a complete flow from start to end. Now I'm using this scenario again to make a high-fidelity prototype. <br/>
The designs and prototypes are both created in Figma. The prototyping tool is limited but good enough for this time. We are still exploring, and nothing has been tested with users yet. There is no need to waste time on minor details unless it's needed to run usability tests.

<figure>
  <img src="/images/ef-007.png">
  <figcaption>The web shows how the screens are connected to each other</figcaption>
</figure>

### Usability testing
Finally, it's time to show our flow to users. During this phase, I worked closely with the product owner. We've planned five meetings with internal and external stakeholders to test the new results dashboard. We created a test script to support us during the usability test—the following things were in the script.
<br/>

**Introduction**: Who are we, why are we doing it, how long does it take, and the goal of this project.<br/>
**Think out loud:** Everything can be valuable for us, don't worry about our feelings, and you can't do wrong.<br/>
**Record:** It would help us if we could record this session. We need to ask permission before we start the recording.<br/>
**Scenario:** describes the context behind a specific user or user group that visits the platform.<br/>
**Hypothesis:** Per scenario, we wrote down the hypothesis. During each task, we asked what the user thought of it.<br/>
**Potential questions:**  Upfront, we prepared lots of questions to validate assumptions we've made.

<figure>
  <img src="/images/ef-008.png">
  <figcaption>We used Microsoft Teams for usability testing</figcaption>
</figure>

### Outcome
After the usability test, everyone was happy! The stakeholders felt heard, and we learned so much. Overall the user was quite pleased, and we're heading in a good direction. We created a shared Miro board to make sense of all the feedback. We added the tested designs and wrote down all the feedback received per screen—Green for positive, Red for negative, Blue for an idea.  

<figure>
  <img src="/images/ef-009.png">
  <figcaption>Grouping feedback from usability test</figcaption>
</figure>


Together with the design team & product owners, we reviewed the importance of the feedback. What do the users like, and what do they hate? Is there overlap in feedback across the different users? We then grouped all feedback into three groups according to the MoSCow prioritization.

After the usability test, I started working on the feedback we prioritized. And soon after, development began with building the foundation of the results dashboard. We will ship this first version confidently, but we're not finished learning. When implemented, we need to continue validating the hypotheses and assumptions.

<figure>
  <img src="/images/ef-010.png">
  <figcaption>The new results dashboard</figcaption>
</figure>
